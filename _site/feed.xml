<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2021-08-14T21:18:15+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Danqing Wang</title><subtitle>王丹青</subtitle><author><name>Danqing Wang</name><email>brxx122@gmail.com</email></author><entry><title type="html">【CALMS】多语言摘要中的信息抽取与共享</title><link href="http://localhost:4000/blogs/CALMS/" rel="alternate" type="text/html" title="【CALMS】多语言摘要中的信息抽取与共享" /><published>2021-08-08T15:32:00+08:00</published><updated>2021-08-08T15:32:00+08:00</updated><id>http://localhost:4000/blogs/CALMS</id><content type="html" xml:base="http://localhost:4000/blogs/CALMS/">&lt;p&gt;Contrastive Aligned Joint Learning for Multilingual Summarization
&lt;strong&gt;&lt;em&gt;Danqing Wang&lt;/em&gt;&lt;/strong&gt;, Jiaze Chen, Hao Zhou, Xipeng Qiu†, Lei Li &lt;br /&gt;
ACL 2021 Findings
论文：&lt;br /&gt;
代码：https://github.com/brxx122/CALMS&lt;br /&gt;
AI科技评论：https://mp.weixin.qq.com/s/DDbpUKiOo1sT6q01deWI3w&lt;/p&gt;

&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;在快节奏的生活中，人们往往没有耐心阅读长篇大论，而是希望能够通过简短的文字迅速掌握文章的重点信息，从而判断是否有必要详细阅读。因此不管是在新闻推送还是在论文撰写，为文章提取一个简明扼要的摘要都是十分重要的。以往这项工作都是由文章作者或者专门的编辑进行，而现在，人们可以通过 AI 模型自动为文章提供摘要，大大解救了为总结全文而绞尽脑汁的作者编辑们。&lt;/p&gt;

&lt;p&gt;紧随国际化的步伐，我们对于摘要生成的需求也不再局限于单种语言。对于我们熟悉的中文，阅读摘要自然能够节约部分时间，但是对于不熟悉的英法德等语言，我们更需要通过摘要来判断是否有必要花费大量精力对全文进行翻译阅读。然而，为每一种不熟悉的语言建立一个模型实在是过于繁重，我们最希望的是有一个统一的模型，能够同时对多种语言的文章进行阅读理解，同时生成对应语言的摘要输出，这就是多语言摘要的研究核心。&lt;/p&gt;

&lt;p&gt;一个优秀的模型除了精心的算法设计，还离不开大量的数据。由于摘要本身撰写难度，人们很难收集到大量高质量的文章-摘要对数据，这个现象在小众的语言上尤为突出。因此，要解决多语言摘要问题，我们首先需要解决的是数据问题。有了数据之后，我们希望能够让模型取长补短，利用资源丰富的语言数据来扶贫资源稀缺的语言。&lt;/p&gt;

&lt;p&gt;这里为大家介绍一篇来自 ACL2021 Findings 的多语言摘要工作《Contrastive Aligned Joint Learning for Multilingual Summarization》。&lt;/p&gt;

&lt;p&gt;该篇文章由字节跳动人工智能实验室和复旦大学合作完成，主要提供了一个囊括了12种语言，总数据量达到100万的新多语言数据集 MLGSum。同时，该篇工作设计了两个任务来提取文章信息并在多种语言间进行语义对齐，从而来同时提升模型在多种语言上的摘要性能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CALMS/0.webp&quot; alt=&quot;论文标题&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;多语言摘要数据集-mlgsum&quot;&gt;多语言摘要数据集 MLGSum&lt;/h2&gt;

&lt;p&gt;机器学习模型，算法为主，但数据先行。没有高质量的大规模数据只能是巧妇难为无米之炊。然而，目前绝大多数摘要数据集均集中在英文上，最近提出的多语言数据集MLSUM[1]也只提供了5种语言。&lt;/p&gt;

&lt;p&gt;因此，作者首先从多语言新闻网站上收集了大量的新闻数据并进行筛选，保留包含人工摘要的部分数据，最终获得了包括 12 种语言，总共文章-摘要对高达100万的大规模数据集。具体语言和数据分布见图 1，其中纵坐标单位为万。&lt;/p&gt;

&lt;p&gt;通过柱状图可以看到，德语（De），英语（En），俄罗斯语（Ru），法语（Fr）和中文（Zh）的数据量较多，其余几种语言的数据量较少。因而作者以此为划分，前面5种作为高资源语种，后面7种作为低资源语种。&lt;/p&gt;

&lt;p&gt;作者的目标在于，在高资源语种上训练一个联合模型，使得其能够同时在5种语言上获得优于单语言模型的性能。与此同时，该联合模型能够很好地迁移到低资源语种上。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CALMS/1.webp&quot; alt=&quot;图1：MLGSum的各语言数据量&quot; /&gt;&lt;/p&gt;
&lt;center&gt;图1：MLGSum的各语言数据量&lt;/center&gt;

&lt;h2 id=&quot;对比学习的多语言摘要模型-calms&quot;&gt;对比学习的多语言摘要模型 CALMS&lt;/h2&gt;

&lt;p&gt;针对摘要的任务特性，作者利用对比学习的思想，设计了两个句子级别的辅助任务。&lt;/p&gt;

&lt;p&gt;第一个叫 &lt;strong&gt;对比句子排序(Contrastive Sentence Ranking, CSR)&lt;/strong&gt;，其目的是帮助模型分辨出哪些信息更加重要。&lt;/p&gt;

&lt;p&gt;具体做法是，首先从文章中随机抽取出若干个句子作为摘要候选；其次将这些候选项和标准摘要进行对比，相似度最高的作为正样本，其余作为负样本。在模型学习过程中，需要将正负样本的表示距离不断拉大，从而分辨出文章中哪些句子对摘要更加重要。&lt;/p&gt;

&lt;p&gt;第二个叫 &lt;strong&gt;对齐句替换 (Sentence Aligned Substitution, SAS)&lt;/strong&gt;，其目的是拉近不同语言间相似句子的距离。&lt;/p&gt;

&lt;p&gt;具体来说，首先作者从语言A的文章中抽取出一些重要信息句（如前几句），翻译成另一种语言B并且进行替换，模型需要根据替换后的混合文章将原始句子还原出来。这个任务希望能够借助翻译拉近语种间的语义表示。从一方面来说，还原的过程可以认为是对重要信息句做B到A的翻译；从另一个方面来说，可以将其视作利用A文章的剩余内容来还原重要信息句。基于重要信息句的信息量和剩余所有内容的信息量之和相似的假设，可以将这个过程视作自监督摘要。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CALMS/2.webp&quot; alt=&quot;图2: CSR和SAS任务设计&quot; /&gt;&lt;/p&gt;
&lt;center&gt;图2: CSR和SAS任务设计&lt;/center&gt;

&lt;h2 id=&quot;性能一览&quot;&gt;性能一览&lt;/h2&gt;

&lt;p&gt;作者利用 mBART 模型[2]作为多语言语言模型初始化，并且利用上述两个任务进行进一步微调，最终获得了模型CALMS（Contrastive Aligned Joint Learning for Multilingual Summarization）。&lt;/p&gt;

&lt;p&gt;首先在5种高资源语言上进行了实验，结果如下所示。其中Mono模型为每种语言一个的单语言模型，Multi模型为联合的多语言模型。可以看出，通过上述两个方法的设计，联合模型在每种语言上都优于单语言模型，并且通过针对每种语言的微调可以进一步提升性能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CALMS/3.webp&quot; alt=&quot;图3: 各模型在De En Ru Fr Zh五种语言上的性能(以ROUGE-1为例)&quot; /&gt;&lt;/p&gt;
&lt;center&gt;图3: 各模型在De En Ru Fr Zh五种语言上的性能(以ROUGE-1为例)&lt;/center&gt;

&lt;p&gt;此外，针对低资源语言，作者将上述在5种语言上联合训练的模型 CALMS 作为初始化，迁移到低资源语言上。其中 Transformer 和 mBART 为直接在该低资源语言上训练的模型。&lt;/p&gt;

&lt;p&gt;可以看到，针对上述5种语言较为相近的几个语系，如Romance罗曼语(Fr Pt Es 法语 葡萄牙语 西班牙语)和Savic斯拉夫语(Ru Uk 俄语 乌克兰语)，CALMS明显优于直接训练的单语言模型，但是对于较远的几个语系，效果有所下降。这是因为CALMS针对上述5个语种进行针对性微调优化，导致语义空间和其余语系更远。同时针对没有被mBART覆盖的Id印度尼西亚语，CALMS取得了优于单语言模型的效果，这是因为CALMS对摘要任务本身提取重要信息的能力也进行了加强。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CALMS/4.webp&quot; alt=&quot;图4: CALMS迁移到低资源语言上的性能&quot; /&gt;&lt;/p&gt;
&lt;center&gt;图4: CALMS迁移到低资源语言上的性能&lt;/center&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;该篇文章为了解决多语言摘要问题，首先提出了一个包含 12 种语言的摘要数据集 MLGSum；其次针对多语言和摘要两个特性设计了两个辅助任务，来加强模型提取重要信息和语言间对齐的能力。最终联合模型CALMS在5种高资源语言上取得了优于单语言模型的能力，并且证实了其在相似语系中有着良好的迁移能力。&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;

&lt;p&gt;[1]Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2020. Mlsum: The multilingual summarization corpus. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8051–8067.&lt;br /&gt;
[2] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726–742.&lt;/p&gt;</content><author><name>Danqing Wang</name><email>brxx122@gmail.com</email></author><category term="blogs" /><category term="Summarization" /><category term="Multilingual" /><category term="Dataset" /><summary type="html">Contrastive Aligned Joint Learning for Multilingual Summarization Danqing Wang, Jiaze Chen, Hao Zhou, Xipeng Qiu†, Lei Li ACL 2021 Findings 论文： 代码：https://github.com/brxx122/CALMS AI科技评论：https://mp.weixin.qq.com/s/DDbpUKiOo1sT6q01deWI3w</summary></entry><entry><title type="html">【CNewSum】具有准确可推断性标注的中文摘要数据集</title><link href="http://localhost:4000/blogs/CNewSum/" rel="alternate" type="text/html" title="【CNewSum】具有准确可推断性标注的中文摘要数据集" /><published>2020-08-14T15:32:00+08:00</published><updated>2020-08-14T15:32:00+08:00</updated><id>http://localhost:4000/blogs/CNewSum</id><content type="html" xml:base="http://localhost:4000/blogs/CNewSum/">&lt;p&gt;软文&lt;/p&gt;</content><author><name>Danqing Wang</name><email>brxx122@gmail.com</email></author><category term="blogs" /><category term="Summarization" /><category term="Dataset" /><summary type="html">软文</summary></entry><entry><title type="html">【HeterSumGraph】异质图神经网络的抽取式摘要模型</title><link href="http://localhost:4000/blogs/HSG/" rel="alternate" type="text/html" title="【HeterSumGraph】异质图神经网络的抽取式摘要模型" /><published>2020-05-06T15:32:00+08:00</published><updated>2020-05-06T15:32:00+08:00</updated><id>http://localhost:4000/blogs/HSG</id><content type="html" xml:base="http://localhost:4000/blogs/HSG/">&lt;p&gt;Heterogeneous Graph Neural Networks for Extractive Document Summarization (ACL 2020)&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Danqing Wang&lt;/em&gt;&lt;/strong&gt;, Pengfei Liu, Yining Zheng, Xipeng Qiu, Xuanjing Huang&lt;br /&gt;
论文：https://arxiv.org/abs/2004.12393&lt;br /&gt;
代码：https://github.com/dqwang122/HeterSumGraph&lt;br /&gt;
知乎：https://zhuanlan.zhihu.com/p/138600416&lt;/p&gt;

&lt;p&gt;抽取式摘要的目标是从原文章中选出最为重要的若干个句子，并且将它们重组成摘要。因而，如何构建句子之间的关系，并得到更好的句子表示，就成为抽取式摘要的核心问题。而本文就试图通过引入词结点来扩充句子间的关系，以异构图的方式来建模抽取式摘要，模型被命名为HeterSumGraph (Heterogeneous Summarization Graph)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic4.zhimg.com/80/v2-4b0c8eb7033b2d7f7ffdb50cfa7d5c97_1440w.jpg&quot; alt=&quot;图1：HeterSumGraph&quot; /&gt;&lt;/p&gt;
&lt;center&gt;图1：HeterSumGraph&lt;/center&gt;

&lt;h2 id=&quot;建模句间关系&quot;&gt;建模句间关系&lt;/h2&gt;
&lt;p&gt;在摘要任务上，建模句间关系的方法可以分成两大类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;以RNN(LSTM)为代表的序列模型&lt;/li&gt;
  &lt;li&gt;以Graph为核心结构的模型&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;序列模型较难捕捉到句子级别的长距离依赖，并且它过于依赖句子上下文的局部信息。相对而言，基于全局信息的图结构更加适合摘要任务。早在2004年，图结构就被用于抽取式摘要任务上：LexRank[1]和TextRank[2]以句子为结点，按照句子之间特征的相似度建边，以无监督迭代的方式对结点进行重要性排序，选出最重要的若干个结点作为摘要。然而，对于以相似度建边的图来说，选择合适的阈值并不容易。近来，一些工作试图通过人工定义的特征来判断句子结点之间是否应该连边（如ADG[3]），或者通过修辞手法或者共同指代等关系来构建图（如RST[4]）。还有的试图直接使用全连接图Transformer，让模型自己学习边权。但是这些图都局限于句子这一种结点，没有引入更多的结点信息。&lt;/p&gt;

&lt;p&gt;而这篇文章试图通过引入词结点来丰富图结构，更好地建立句子之间的关系。词结点的引入基于以下几方面的考虑：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;目前的抽取式摘要系统更多依赖于句子的位置信息，句子的内容信息并没有得到很好的编码[5]。甚至在模型输入时，删除句子里面的名词、动词、形容词等等，都对最终结果的影响不大[6]。引入词结点，并且使它们和句子结点反复迭代更新，能够加强词在句子表示中的作用。&lt;/li&gt;
  &lt;li&gt;通过共同出现的词，句子之间的关系得到了扩充。早期依靠相似度建边的图结构，本质也是依赖于句子之间内容的重叠程度。引入词结点后，模型不再需要手动确定相似度的阈值，词和句子之间的包含关系是确定的，而拥有越多相同词的句子间关系越紧密。同时，句子之间的关系不再是单一的连边/不连边，而是根据词的不同有不同的关系。&lt;/li&gt;
  &lt;li&gt;因为词是最小的语义单元，因此它可以作为中介结点链接任何比它大的语义单元。作为句子的中介，它可以更好地建立句子间的关系；作为文章的中介，它同样可以建立多文档关系。因此，模型可以很轻易地从单文档摘要迁移到多文档摘要任务上。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;hetersumgraphhsg&quot;&gt;HeterSumGraph（HSG）&lt;/h2&gt;

&lt;p&gt;HeterSumGraph的结构如上图1所示，主要由三部分构成：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;图初始化模块&lt;/li&gt;
  &lt;li&gt;异质图层迭代更新&lt;/li&gt;
  &lt;li&gt;句子选择模块&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;图初始化模块分别对词结点、句子结点以及词和句子的连边进行初始化，其中句子结点分别使用了CNN和LSTM进行内容和位置信息的编码，而连边选用TF-IDF特征作为权重。&lt;/p&gt;

&lt;p&gt;异质图层的更新分成两个方向：词到句子和句子到词。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic4.zhimg.com/80/v2-1557ddcb770fde2c1cfac4ee329b8fe3_1440w.jpg&quot; alt=&quot;图2：词结点和句子结点的迭代更新&quot; /&gt;&lt;/p&gt;
&lt;center&gt;图2：词结点和句子结点的迭代更新&lt;/center&gt;

&lt;p&gt;词到句子给了句子结点更好的内容表示，句子到词的更新为词结点提供其出现次数的统计信息，从而使得多次出现的重要词语得到更好的更新。进一步，这个信息将会通过词到句子的再次迭代传递给句子，使得拥有更多重点词语的句子得到更好的表示。这种通过结点度数而得到的频数信息，是图结构区别于基于上下文编码的序列模型的重要特征之一。&lt;/p&gt;

&lt;p&gt;句子选择模块主要是对句子进行重要性排序，并且尝试了一些朴素的去冗余操作，如Trigram blocking。&lt;/p&gt;

&lt;p&gt;通过添加文章结点，可以从单文档任务迁移到多文档摘要上，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic4.zhimg.com/80/v2-1dbd960795e269d12037d93529daa2b7_1440w.jpg&quot; alt=&quot;图3：多文档摘要模型HeterDocSumGraph的图结构&quot; /&gt;&lt;/p&gt;
&lt;center&gt;图3：多文档摘要模型HeterDocSumGraph的图结构&lt;/center&gt;

&lt;h2 id=&quot;实验与分析&quot;&gt;实验与分析&lt;/h2&gt;
&lt;p&gt;HSG分别在单文档和多文档的三个摘要数据集上进行了测试。单文档摘要选择了较为常见的CNN/DailyMail和NYT50数据集，多文档则选择了ACL2019 Fabbri提出的Multi-News[7]：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic4.zhimg.com/80/v2-676a93bd9d4df2366c210ca5f62d4dcf_1440w.jpg&quot; alt=&quot;表1：单文档摘要集CNN/DailyMail和NYT50结果&quot; /&gt;&lt;/p&gt;
&lt;center&gt;表1：单文档摘要集CNN/DailyMail和NYT50结果&lt;/center&gt;

&lt;p&gt;&lt;img src=&quot;https://pic3.zhimg.com/80/v2-980a697ad6c0ff50eec144da4d21e76e_1440w.jpg&quot; alt=&quot;表2：多文档摘要集Multi-News结果&quot; /&gt;&lt;/p&gt;
&lt;center&gt;表2：多文档摘要集Multi-News结果&lt;/center&gt;

&lt;p&gt;那么这种收益是什么带来的呢？除了简单的消融实验之外，文章还进行了进一步探究。作者认为，如果引入词结点以及词语出现频率（即词结点度数）是有帮助的话，那么对于词结点平均度数越高的图，收益越是明显。换言之，如果文章中每个词都只出现过一次，那么得到图结构其实和序列模型差别不大，只有在存在多次出现的词语的文章中，词结点才能够获得多个句子的更新。因此文章按照词结点的平均度数对CNN/DM测试集进行了划分，以折线表示BiLSTM和HSG模型的性能，以柱状图表示两个模型的性能差值：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/80/v2-91b12dd4450ccb8a3fd2b57d56045bec_1440w.jpg&quot; alt=&quot;图4：平均词结点度数和多文档输入文章数的探究性实验&quot; /&gt;&lt;/p&gt;
&lt;center&gt;图4：平均词结点度数和多文档输入文章数的探究性实验&lt;/center&gt;

&lt;p&gt;可以看到在词结点平均度数越高的区间上，两个模型的性能差值越明显。因此可以验证，HSG引入词结点带来的优势主要在于多个句子对词结点的更新。&lt;/p&gt;

&lt;p&gt;此外，文章还对多文档任务进行了探究。通过对输入文档个数对加/不加文章结点的图模型性能探究，验证了引入文章结点来构建文章之间的关系对多文档摘要是非常重要的，并且随着源文档数目的增加，这个影响更加明显。&lt;/p&gt;

&lt;h2 id=&quot;引用文献&quot;&gt;引用文献&lt;/h2&gt;
&lt;p&gt;[1] Erkan, G., &amp;amp; Radev, D. R. (2004). LexRank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research, 22, 457–479.&lt;br /&gt;
[2] Mihalcea, R., &amp;amp; Tarau, P. (2004). TextRank: Bringing Order into Texts, 45(4).&lt;br /&gt;
[3] Yasunaga, M., Zhang, R., Meelu, K., Pareek, A., Srinivasan, K., &amp;amp; Radev, D. (2017). Graph-based Neural Multi-Document Summarization. CoNLL.&lt;br /&gt;
[4] Xu, J., Gan, Z., Cheng, Y., &amp;amp; Liu, J. (2019). Discourse-Aware Neural Extractive Model for Text Summarization&lt;br /&gt;
[5] Zhong, M., Liu, P., Wang, D., Qiu, X., &amp;amp; Huang, X. (2019). Searching for Effective Neural Extractive Summarization: What Works and What’s Next, 1049–1058. ACL&lt;br /&gt;
[6] Kedzie, C., Mckeown, K., &amp;amp; Daum, H. (2018). Content Selection in Deep Learning Models of Summarization. In Empirical Methods in Natural Language Processing (EMNLP).&lt;br /&gt;
[7] Fabbri, A. R., Li, I., She, T., Li, S., &amp;amp; Radev, D. R. (2019). Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model. In ACL. Retrieved from http://arxiv.org/abs/1906.0174&lt;/p&gt;</content><author><name>Danqing Wang</name><email>brxx122@gmail.com</email></author><category term="blogs" /><category term="Summarization" /><category term="Graph Neural Networks" /><summary type="html">Heterogeneous Graph Neural Networks for Extractive Document Summarization (ACL 2020) Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, Xuanjing Huang 论文：https://arxiv.org/abs/2004.12393 代码：https://github.com/dqwang122/HeterSumGraph 知乎：https://zhuanlan.zhihu.com/p/138600416</summary></entry></feed>