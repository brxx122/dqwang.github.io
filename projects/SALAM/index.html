<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Learning from Mistakes via Cooperative Study Assistant for Large Language Models </title>

  <link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico">

  <link href="https://cdn.bootcdn.net/ajax/libs/twitter-bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">


  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">

  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <link rel="stylesheet" href="/assets/dl.css">

  <!-- Custom Fonts -->
  <!-- <link href="//netdna.bootstrapcdn.com/font-awesome/3.1.1/css/font-awesome.css" rel="stylesheet"> -->





  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <!-- jQuery -->
  <script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
  <!-- Bootstrap Core JavaScript -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@3.1.1/dist/js/bootstrap.min.js"></script>

</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning from Mistakes via Cooperative Study Assistant for Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="https://dqwang122.github.io/" target="_blank">Danqing Wang</a>,
            </span>
            <span class="author-block">
              <a href="https://lileicc.github.io/" target="_blank">Lei Li</a><sup>‚Ä†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">UC Santa Barbara, ‚Ä†Carnegie Mellon University<br>
              EMNLP 2023
            </span>
            <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
               <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2305.13829" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary PDF link -->
              <!-- <span class="link-block">
                <a href="https://drive.google.com/file/d/1i9xfOkQ60kixj0rZ-kCo8UCo2fZ51fCY/view?usp=sharing" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-google-drive"></i>
                </span>
                <span>Dataset</span>
                </a>
              </span> -->

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/dqwang122/SALAM" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
              </span>

              <!-- Slides -->
              <span class="link-block">
                <a href="/assets/PPT/20231027_EMNLP2023_SALAM.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-powerpoint"></i>
                </span>
                <span>Slides</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here
        <source src="static/videos/salam_demo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <strong>Cooperative Interaction between two agents</strong>: <em style="color:#3393FF;font-family: 'PT Sans'">main LLM</em> and <em style="color:#3393FF;font-family: 'PT Sans'">study assistant</em>.
        The study assistant helps LLM revise its response by analyizing its previous mistakes and providing guidelines based on the ground truth.
        It also maintains a <em style="color:#FF8333; font-family: 'PT Sans'">mistake memory</em> for all mistakes the LLM made on the training set.
        During inference, the study assistant directly provides guidance without the ground truth and retrieves similar mistakes from the collection.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose <strong>Study Assistant for Large LAnguage Model (SALAM)</strong>, a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation. In the gathering phase, the student assistant agent probes the main LLM, analyzes its errors, and collects the interaction in a mistake memory. During the examination phase, the study assistant provides guidelines by retrieving relevant cases to help the main LLM anticipate and avoid similar errors. We first investigate the effectiveness of a general study assistant and then customize it to provide LLMspecific guidance through imitation learning from successful guidance experiences. Our experiments on three LLMs using two challenging frameworks demonstrate that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on BBQ.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper content -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-half">
        <h4 class="title is-4">Cooperation Makes LLM Better</h4>
        <div>
            <img src="static/images/overview.jpg" width="100%" height="100%" alt="SALAM">
        </div>
        <br>
        <div class="content has-text-justified">
          <p><strong>üò® LLM may self-reflect, but is the reflection always reliable and reusable?</strong></p>
          <ul>
          <li>Inopportune time to stop or continue the refinement loop</li>
          <li>Too vague feedback to refine response</li>
          <li>Repeated mistakes without knowing previous reflection</li>
          </ul>
          <p><strong>üßê We need an expert to help LLMs reflect</strong></p>
          <ul>
          <li>analyze common misunderstanding and provide global guidelines</li>
          <li>collect the experience for future use</li>
          </ul>
        </div>
      </div>

        <div class="column is-half">
          <h4 class="title is-4">Mistake Gathering & Examination</h4>
          <div>
              <img src="static/images/memory.jpg" width="90%" height="90%" alt="Mistake Memory">
          </div>
          <br>
          <div class="content has-text-justified">
            <p><strong>üìö Mistake Memory uiltizes previous mistakes</strong></p>
            <ul>
            <li>Collect mistakes from the training data with ground truth</li>
            <li>Let main LLM interact with the study assistant until it gets the correct answer</li>
            <li>Store these experience into mistake memory</li>
            </ul>
            <p><strong>üìù No ground truth is provided during Examination</strong></p>
            <ul>
            <li>Study assistant retrieves similar mistakes from the memory and provides guideline without the ground truth</li>
            </ul>
          </div>
        </div>
      </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <div class="content has-text-justified">
        <h3 class="title is-4">Train a model-agnostic Study Assistant</h4>
          <h5 class="title is-5", style="color:#3393FF;font-family: 'PT Sans'"><em>Focus only on the case, not the LLM!</em></h5>
          <ol>
          <li>Dataset: Collect mistakes from different LLMs and get feedback data from GPT-4</li>
            <ul>
              <li> Analysis (why is wrong) </li>
              <li> Guideline (how to avoid) </li>
            </ul>
          <li>Training: finetune a LLaMA-based study assistant on the feedback dataset.</li>
          </ol>
        </div>

        <div class="content has-text-justified">
        <h3 class="title is-4">Tailor a model-specific Study Assistant for each LLM</h4>
          <h5 class="title is-5", style="color:#3393FF;font-family: 'PT Sans'"><em>Each LLM has his own opinion!</em></h5>
          <ol>
          <li>Formulation: Markov decision process</li>
          <ul>
            <li><strong>State S</strong>: (query, response, context)</li>
            <li><strong>Action A</strong>: feedback from the study assistant</li>
            <li><strong>Reward R</strong>: LLM performance
              <ul>
                <li>1 if the LLM‚Äôs response is correct</li>
                <li>0 otherwise</li>
              </ul>
            </li>
            <li><p><strong>Policy ùúã(ùëé|ùë†)</strong>: a language model to provide feedback</p>
            </li>
          </ul>
          <li>Offline Sampling: Collect a replay dataset and only keep successful trajectories.</li>
          <li>Training: finetune a LLaMA-based study assistant on the filtered replay dataset.</li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper content -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <!-- <div class="item">
        <div style="text-align: center;">
          <img src="static/images/overview.jpg" width="80%" height="80%" alt="overview"/>
        </div>
        <h2 class="subtitle has-text-centered">
          SALAM. There are two agents in SALAM framework: an LLM and a study assistant. The blue contents are generated by LLM and the green ones are from the study assistant. The LLM has a first try on a set of queries and passes its response to the study assistant.
          The assistant grades the response based on the ground truth and collect mistakes. It then returns feedback and asks LLM to refine its answer. For inference, the study assistant directly retrieves relevant mistakes for the new query and generates the analysis and guidelines as additional instruction for LLM.
        </h2>
      </div> -->
      <div class="item">
        <!-- Your image here -->
        <div style="text-align: center;">
          <img src="static/images/results.jpg" alt="main results"/>
        </div>
        <h2 class="subtitle has-text-centered">We tested SALAM on 27 tasks from BBH & BBQ for Flan-T5 11B, LLaMA-1 7B, and GPT-NeoX 20B. It boosted their performance without fine-tuning on these tasks.</h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <div style="text-align: center;">
          <img src="static/images/gpt4.jpg" alt="gpt4 results"/>
        </div>
        <h2 class="subtitle has-text-centered">We take GPT-4 as our main LLM and study assistant respectively. GPT-4 can benefits from our small finetuned LLaMA-based study assistant. As the study assistant Although it is more powerful than the small study assistant, our study assistant can easily generate more feedback with no cost and outperform it.</h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <div style="text-align: center;">
          <img src="static/images/retrieval.jpg" alt="retrieval"/>
        </div>
        <h2 class="subtitle has-text-centered">The investigation of retrieval. We conduct experiments on BBQ with Flan-T5. SALAM benefits from the precise retrieval.</h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      Paper video.
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <div class="publication-video">
            Youtube embed code here
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            Your video file here
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            Your video file here
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            Your video file here
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>

      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{wang2023learn,
      title={Learn from Mistakes through Cooperative Interaction with Study Assistant},
      author={Wang, Danqing and Li, Lei},
      journal={The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP) 2023},
      year={2023}
}
    </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
